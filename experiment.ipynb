{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c19e1cbc",
   "metadata": {},
   "source": [
    "Natural Language Processing\n",
    "\n",
    "Why natural language came in to existence?\n",
    "\n",
    "So machine can predict if we give some values or so kind of input data but if the data is in text format the model does not know any luanguage how can it predict the output thats where this NLP came into existance.\n",
    "\n",
    "Use cases:\n",
    "1. Spelling mistakes are automatically corrected when we use this models.\n",
    "2. Automatic replies according to resent conversation.\n",
    "3. text translation-> see translation.\n",
    "4. Alexa and google assistance are the best examples.\n",
    "\n",
    "It became day to day activity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3af00a",
   "metadata": {},
   "source": [
    "Tokenization in NLP:\n",
    "\n",
    "1. corpus - Paragraph is called corpus.\n",
    "2. documents - sentences is called docuemnts.\n",
    "3. vocobulary - unique words.\n",
    "\n",
    "Tokenization:\n",
    "\n",
    "My name is sandy. I love teaching. - corpus\n",
    "\n",
    "Tokenization->{senteces or documents}\n",
    "\n",
    "1->My name is sandy.\n",
    "2->I love teaching.\n",
    "\n",
    "Tokenization -> words \n",
    "\n",
    "1. My\n",
    "2. name\n",
    "3. is\n",
    "4. Sandy \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0453458d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome my name is kadha jyothik sandeep. I was learnign Natural language processing.\n",
      "I love learning new things.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus=\"\"\"Hello Welcome my name is kadha jyothik sandeep. I was learnign Natural language processing.\n",
    "I love learning new things.\n",
    "\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edecdc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sandeep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "314b98c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', download_dir='C:/nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40c0dd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sandeep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt', download_dir=r\"C:\\Users\\Sandeep\\AppData\\Roaming\\nltk_data\")\n",
    "nltk.data.path.append(r\"C:\\Users\\Sandeep\\AppData\\Roaming\\nltk_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47176fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55252037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85d7985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP is amazing.', 'It helps computers understand human language!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(r\"C:\\Users\\Sandeep\\AppData\\Roaming\\nltk_data\")\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "corpus = \"NLP is amazing. It helps computers understand human language!\"\n",
    "print(sent_tokenize(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac90fbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello my name is kadha.', 'I love studying']\n"
     ]
    }
   ],
   "source": [
    "# sentence into paragraph\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "data=\"\"\"hello my name is kadha. I love studying\"\"\"\n",
    "documents=sent_tokenize(data)\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f85bc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'my', 'name', 'is', 'kadha', '.']\n",
      "['I', 'love', 'studying']\n"
     ]
    }
   ],
   "source": [
    "# paragraph into words or sentence into words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "# curpus to word\n",
    "word_tokenize(data)\n",
    "\n",
    "\n",
    "#docuemnt into word\n",
    "\n",
    "for i in documents:\n",
    "    print(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6154aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "# this will also recognize the puncuation marks and give us the result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60762723",
   "metadata": {},
   "source": [
    "Stemming is a text normalization technique used in NLP to reduce a word to its base/root form by cutting off its ends.\n",
    "\n",
    "It does not care about grammar or dictionary correctness â€” it just chops the word down.\n",
    "\n",
    "| Original Word | Stemmed Form             |\n",
    "| ------------- | ------------------------ |\n",
    "| running       | run                      |\n",
    "| runs          | run                      |\n",
    "| runner        | runner (sometimes â€œrunâ€) |\n",
    "| playing       | play                     |\n",
    "| played        | play                     |\n",
    "| studies       | studi                    |\n",
    "| studying      | study                    |\n",
    "\n",
    "\n",
    "Why stemming is used?\n",
    "\n",
    "To reduce vocabulary size by grouping similar words:\n",
    "\n",
    "â€œrunâ€, â€œrunningâ€, â€œrunsâ€ â†’ run\n",
    "\n",
    "â€œstudyâ€, â€œstudyingâ€, â€œstudiesâ€ â†’ studi\n",
    "\n",
    "This makes it easier for ML models to treat variations as the same word.\n",
    "\n",
    "Example:\n",
    "\n",
    "\"I am running\"\n",
    "\"I run\"\n",
    "\"I ran\"\n",
    "\n",
    "\n",
    "All describe the same action â†’ stemmer makes it consistent.\n",
    "\n",
    "ðŸ§  Types of Stemmers\n",
    "1ï¸âƒ£ Porter Stemmer (most common)\n",
    "\n",
    "Fast, simple, cuts endings aggressively.\n",
    "\n",
    "2ï¸âƒ£ Snowball Stemmer\n",
    "\n",
    "Improved version of Porter.\n",
    "\n",
    "3ï¸âƒ£ Lancaster Stemmer\n",
    "\n",
    "Very aggressive; often chops too much.\n",
    "\n",
    "ðŸ§ª Example in Python (NLTK)\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"runs\", \"ran\", \"studies\", \"studying\"]\n",
    "\n",
    "for w in words:\n",
    "    print(w, \"â†’\", stemmer.stem(w))\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "running â†’ run\n",
    "runs â†’ run\n",
    "ran â†’ ran\n",
    "studies â†’ studi\n",
    "studying â†’ study\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cedaa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running â†’ run\n",
      "runs â†’ run\n",
      "ran â†’ ran\n",
      "studies â†’ studi\n",
      "studying â†’ studi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"runs\", \"ran\", \"studies\", \"studying\"]\n",
    "\n",
    "for w in words:\n",
    "    print(w, \"â†’\", stemmer.stem(w))\n",
    "\n",
    "\n",
    "# disadvangtage is studi is not a word here the meaning is changing we need to have a look on it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8341b",
   "metadata": {},
   "source": [
    "Regex Stemmer means you write your own stemming rules using regular expressions instead of using pre-built stemmers like Porter or Snowball.\n",
    "\n",
    "It gives full control over how words are reduced.\n",
    "\n",
    "âœ… What is a Regex Stemmer?\n",
    "\n",
    "A Regex Stemmer uses pattern matching (regex) to remove common suffixes like:\n",
    "\n",
    "ing\n",
    "\n",
    "ed\n",
    "\n",
    "s\n",
    "\n",
    "ly\n",
    "\n",
    "Example:\n",
    "\"playing\" â†’ remove \"ing\" â†’ \"play\"\n",
    "\"studied\" â†’ remove \"ied\" â†’ \"y\"\n",
    "\n",
    "This is helpful if you want simple, custom stemming rules.\n",
    "\n",
    "ðŸ§© NLTK has a built-in RegexStemmer class\n",
    "\n",
    "You can create a stemmer by defining a regex pattern that removes suffixes.\n",
    "\n",
    "ðŸ”§ Example: Create a Regex Stemmer in Python\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "# remove common endings: ing, ly, s, ed\n",
    "stemmer = RegexpStemmer('ing$|ly$|s$|ed$', min=3)\n",
    "\n",
    "words = [\"running\", \"plays\", \"studied\", \"likely\", \"cars\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word, \"â†’\", stemmer.stem(word))\n",
    "\n",
    "Output:\n",
    "running â†’ run\n",
    "plays â†’ play\n",
    "studied â†’ studi\n",
    "likely â†’ like\n",
    "cars â†’ car\n",
    "\n",
    "ðŸ” How the Regex works\n",
    "\n",
    "The pattern:\n",
    "\n",
    "ing$|ly$|s$|ed$\n",
    "\n",
    "\n",
    "Means:\n",
    "\n",
    "ing$ â†’ remove \"ing\" from end of word\n",
    "\n",
    "ly$ â†’ remove \"ly\"\n",
    "\n",
    "s$ â†’ remove \"s\"\n",
    "\n",
    "ed$ â†’ remove \"ed\"\n",
    "\n",
    "| means OR\n",
    "\n",
    "min=3 prevents words shorter than 3 letters from being trimmed.\n",
    "\n",
    "ðŸ§  Why use Regex Stemmer?\n",
    "\n",
    "âœ” Fully customizable\n",
    "âœ” Good for simple language tasks\n",
    "âœ” Faster than full algorithms\n",
    "âœ” You control exactly how words should be trimmed\n",
    "\n",
    "ðŸ›‘ Downsides\n",
    "\n",
    "Not language-aware\n",
    "\n",
    "Can break words incorrectly\n",
    "\n",
    "Not as accurate as Porter/Snowball/Lemmatization\n",
    "\n",
    "Example:\n",
    "\"bus\" â†’ bu (because it ends with \"s\")\n",
    "\n",
    "ðŸŽ¯ Summary\n",
    "\n",
    "Regex Stemmer = Build your own stemmer using simple regex rules.\n",
    "Useful when you want:\n",
    "\n",
    "Lightweight stemming\n",
    "\n",
    "Full control\n",
    "\n",
    "Custom rules for your domain (medicine, legal, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8928598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Regex stemmer class\n",
    "\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "regexstem=RegexpStemmer('ing|s$|e$')\n",
    "\n",
    "regexstem.stem(\"helloing\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf71e37",
   "metadata": {},
   "source": [
    "The Snowball Stemmer is an improved and more aggressive version of the Porter Stemmer.\n",
    "It is one of the most commonly used stemmers in NLP because it is:\n",
    "\n",
    "âœ” Faster\n",
    "âœ” More accurate\n",
    "âœ” Works with many languages\n",
    "âœ” More consistent than Porter\n",
    "\n",
    "â„ï¸ What is Snowball Stemmer?\n",
    "\n",
    "Snowball Stemmer (also called Porter2) is a rule-based algorithm that reduces words to their base/root form by removing suffixes.\n",
    "\n",
    "Example:\n",
    "\n",
    "Word\tSnowball Stem\n",
    "running\trun\n",
    "studies\tstudi\n",
    "beautiful\tbeauti\n",
    "organization\torgan\n",
    "doing\tdo\n",
    "\n",
    "It handles linguistic rules better than Porter.\n",
    "\n",
    "ðŸŒ Languages supported\n",
    "\n",
    "Snowball Stemmer supports many languages:\n",
    "\n",
    "English\n",
    "\n",
    "Danish\n",
    "\n",
    "Dutch\n",
    "\n",
    "French\n",
    "\n",
    "Finnish\n",
    "\n",
    "German\n",
    "\n",
    "Italian\n",
    "\n",
    "Norwegian\n",
    "\n",
    "Portuguese\n",
    "\n",
    "Russian\n",
    "\n",
    "Spanish\n",
    "\n",
    "Swedish\n",
    "\n",
    "Porter works only for English, but Snowball supports many languages.\n",
    "\n",
    "ðŸ”§ Using Snowball Stemmer in Python (NLTK)\n",
    "Example Code:\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "words = [\"running\", \"studies\", \"studying\", \"organization\", \"playing\", \"happiness\"]\n",
    "\n",
    "for w in words:\n",
    "    print(w, \"â†’\", stemmer.stem(w))\n",
    "\n",
    "Output:\n",
    "running â†’ run\n",
    "studies â†’ studi\n",
    "studying â†’ studi\n",
    "organization â†’ organ\n",
    "playing â†’ play\n",
    "happiness â†’ happi\n",
    "\n",
    "ðŸ§  Why Snowball is better than Porter?\n",
    "Feature\tPorter Stemmer\tSnowball Stemmer\n",
    "Accuracy\tMedium\tHigh\n",
    "Aggressiveness\tMedium\tHigh\n",
    "Speed\tFast\tVery fast\n",
    "Languages\tOnly English\tMany\n",
    "Rules\tOlder\tImproved Porter2 rules\n",
    "\n",
    "Snowball Stemmer is considered the default recommended stemmer for English text tasks.\n",
    "\n",
    "ðŸ”¬ Example: Snowball vs Porter comparison\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "word = \"studies\"\n",
    "print(\"Porter:\", porter.stem(word))\n",
    "print(\"Snowball:\", snowball.stem(word))\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "Porter: studi\n",
    "Snowball: studi\n",
    "\n",
    "\n",
    "But with other words:\n",
    "\n",
    "word = \"relational\"\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "Porter: relat\n",
    "Snowball: relat\n",
    "\n",
    "\n",
    "Snowball is generally more consistent.\n",
    "\n",
    "ðŸ§  When should you use Snowball Stemmer?\n",
    "\n",
    "Use Snowball when:\n",
    "\n",
    "âœ” You want a stronger stemmer\n",
    "âœ” You want better accuracy\n",
    "âœ” Youâ€™re working in a non-English language\n",
    "âœ” You want fast performance\n",
    "\n",
    "ðŸŽ¯ Summary\n",
    "\n",
    "Snowball Stemmer = Improved Porter Stemmer (Porter2)\n",
    "Best for real-world NLP tasks because it's:\n",
    "\n",
    "fast\n",
    "\n",
    "multilingual\n",
    "\n",
    "accurate\n",
    "\n",
    "reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fcbbd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "ran\n",
      "studi\n",
      "studi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow=SnowballStemmer('english')\n",
    "\n",
    "words = [\"running\", \"runs\", \"ran\", \"studies\", \"studying\"]\n",
    "\n",
    "for i in words:\n",
    "    print(snow.stem(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dffee8",
   "metadata": {},
   "source": [
    "### Text processing using Lemmatization NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96013c20",
   "metadata": {},
   "source": [
    "Lemmatization is a text normalization technique in NLP that reduces a word to its meaningful base/root form, called a lemma.\n",
    "\n",
    "Unlike stemming, lemmatization uses dictionary-based rules + part-of-speech (POS) understanding, so the result is always a real English word.\n",
    "\n",
    "ðŸŒ± Lemmatization = Meaningful reduction, using vocabulary + grammar\n",
    "\n",
    "Examples:\n",
    "\n",
    "Word\tLemma\n",
    "studies\tstudy\n",
    "studying\tstudy\n",
    "better\tgood\n",
    "mice\tmouse\n",
    "went\tgo\n",
    "having\thave\n",
    "\n",
    "Notice:\n",
    "\n",
    "All lemmas are valid dictionary words\n",
    "\n",
    "Lemmatization understands context + grammar\n",
    "\n",
    "ðŸ†š Stemming vs Lemmatization\n",
    "Feature\tStemming\tLemmatization\n",
    "Output\tNot real words sometimes\tAlways real words\n",
    "Process\tCuts word endings\tUses vocabulary + POS\n",
    "Example\tâ€œstudiesâ€ â†’ studi\tâ€œstudiesâ€ â†’ study\n",
    "Speed\tFaster\tSlower\n",
    "Accuracy\tLower\tHigher\n",
    "\n",
    "Lemmatization is smarter.\n",
    "\n",
    "ðŸ”§ Lemmatization in Python (NLTK)\n",
    "\n",
    "Before using it, download WordNet:\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "Example code:\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"studies\", \"studying\", \"better\", \"mice\", \"running\", \"went\"]\n",
    "\n",
    "for w in words:\n",
    "    print(w, \"â†’\", lemmatizer.lemmatize(w))\n",
    "\n",
    "Output:\n",
    "studies â†’ study\n",
    "studying â†’ studying      (needs POS)\n",
    "better â†’ better          (needs POS)\n",
    "mice â†’ mouse\n",
    "running â†’ running        (needs POS)\n",
    "went â†’ went              (needs POS)\n",
    "\n",
    "\n",
    "Why?\n",
    "Because default lemmatization assumes noun.\n",
    "We must give POS tags for correct results.\n",
    "\n",
    "ðŸ§  Lemmatization with POS Tagging (Better results)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "words = [\"studies\", \"studying\", \"better\", \"mice\", \"running\", \"went\"]\n",
    "\n",
    "for w in words:\n",
    "    lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "    print(w, \"â†’\", lemma)\n",
    "\n",
    "Correct Output:\n",
    "studies â†’ study\n",
    "studying â†’ study\n",
    "better â†’ good\n",
    "mice â†’ mouse\n",
    "running â†’ run\n",
    "went â†’ go\n",
    "\n",
    "ðŸ§© When should you use Lemmatization?\n",
    "\n",
    "Use lemmatization when:\n",
    "\n",
    "âœ” You need meaningful, correct words\n",
    "âœ” Doing text classification\n",
    "âœ” Doing search engines / information retrieval\n",
    "âœ” Doing topic modeling\n",
    "âœ” Want cleaner vocabulary\n",
    "\n",
    "Use stemming when:\n",
    "\n",
    "âœ” Speed matters\n",
    "âœ” Accuracy is not critical\n",
    "\n",
    "ðŸŽ¯ Summary\n",
    "\n",
    "Lemmatization = Reduce words to dictionary form using POS + grammar.\n",
    "\n",
    "It is more accurate than stemming because:\n",
    "\n",
    "Uses vocabulary\n",
    "\n",
    "Uses grammar rules\n",
    "\n",
    "Output is always a real word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53e66aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sandeep\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cdff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n",
      "ran\n",
      "study\n",
      "studying\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "# POS Noun -n, verb -v, adjective -a, adverb -r\n",
    "\n",
    "lemmatizer.lemmatize(\"going\",pos=\"v\")\n",
    "\n",
    "words = [\"running\", \"runs\", \"ran\", \"studies\", \"studying\"]\n",
    "\n",
    "\n",
    "# by fefault it will consider it as noun\n",
    "for i in words:\n",
    "    print(lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5365963a",
   "metadata": {},
   "source": [
    "### Text processing stopwords\n",
    "\n",
    "Stopwords are the common, frequently occurring words in a language that usually do not add meaningful information to text analysis.\n",
    "\n",
    "Examples of English stopwords:\n",
    "\n",
    "the, is, are, am, was, were, a, an, in, on, at, to, for, of, with\n",
    "\n",
    "These words appear in almost every sentence, so they don't help the model understand the text better.\n",
    "\n",
    "ðŸ§  Why do we remove stopwords?\n",
    "\n",
    "To reduce:\n",
    "\n",
    "Noise\n",
    "\n",
    "Vocabulary size\n",
    "\n",
    "Model training time\n",
    "\n",
    "And to keep only the meaningful words.\n",
    "\n",
    "Example sentence:\n",
    "\n",
    "\"I am learning NLP from scratch\"\n",
    "\n",
    "\n",
    "After removing stopwords:\n",
    "\n",
    "[\"learning\", \"NLP\", \"scratch\"]\n",
    "\n",
    "\n",
    "You keep what matters.\n",
    "\n",
    "ðŸ”§ Stopword Removal in Python (NLTK)\n",
    "Step 1 â€” Download stopwords (only once)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "Step 2 â€” Use them\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "sentence = \"I am learning NLP from scratch and it is very interesting!\"\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "filtered_words = [w for w in words if w.lower() not in stop_words]\n",
    "\n",
    "print(\"Original:\", words)\n",
    "print(\"After removing stopwords:\", filtered_words)\n",
    "\n",
    "Output:\n",
    "Original: ['I', ' am', 'learning', 'NLP', 'from', 'scratch', 'and', 'it', 'is', 'very', 'interesting', '!']\n",
    "After removing stopwords: ['learning', 'NLP', 'scratch', 'interesting', '!']\n",
    "\n",
    "ðŸ“Œ Stopwords list\n",
    "\n",
    "You can view the entire stopword list:\n",
    "\n",
    "print(stopwords.words(\"english\"))\n",
    "\n",
    "ðŸ§© When should you NOT remove stopwords?\n",
    "\n",
    "Do NOT remove stopwords when working on:\n",
    "\n",
    "Machine Translation\n",
    "\n",
    "Text Generation\n",
    "\n",
    "Sentiment Analysis (â€œnotâ€ is very important!)\n",
    "\n",
    "Question Answering\n",
    "\n",
    "Grammar analysis\n",
    "\n",
    "Because removing words like â€œnotâ€, â€œisâ€, â€œdoesâ€ changes meaning.\n",
    "\n",
    "Example:\n",
    "\n",
    "\"I am not happy\"\n",
    "\n",
    "\n",
    "After removing stopwords:\n",
    "\n",
    "[\"happy\"]\n",
    "\n",
    "\n",
    "Meaning becomes completely opposite.\n",
    "\n",
    "ðŸŽ¯ Summary\n",
    "\n",
    "Stopwords = Common words that donâ€™t add much meaning.\n",
    "Used to clean text during preprocessing.\n",
    "\n",
    "âœ” Reduces noise\n",
    "âœ” Speeds up training\n",
    "âœ” Makes features more meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "faf0d2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sandeep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text preprocessing stopping\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696d3a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# from nltk.tokenize word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words(\"english\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2711c7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur languag process becom crucial part modern technolog , influenc almost everi digit interact today .', 'from analyz custom review power intellig assist , nlp help machin understand context , emot , intent behind human commun .', 'over past decad , compani heavili invest nlp research build smarter chatbot , sentiment analysi tool , translat engin , autom content system .', 'what make nlp challeng complex human languageâ€”word multipl mean , grammar rule inconsist , peopl express idea countless way .', 'despit challeng , rapid advanc deep learn larg languag model significantli improv accuraci .', 'as busi adopt nlp-driven solut , futur promis highli person experi machin understand say , also feel need .']\n"
     ]
    }
   ],
   "source": [
    "data=\"\"\"Natural Language Processing has become a crucial part of modern technology, influencing almost every digital interaction we have today. From analyzing customer reviews to powering intelligent assistants, NLP helps machines understand the context, emotion, and intent behind human communication. Over the past decade, companies have heavily invested in NLP research to build smarter chatbots, sentiment analysis tools, translation engines, and automated content systems. What makes NLP challenging is the complexity of human languageâ€”words can have multiple meanings, grammar rules can be inconsistent, and people express ideas in countless ways. Despite these challenges, rapid advancements in deep learning and large language models have significantly improved accuracy. As more businesses adopt NLP-driven solutions, the future promises highly personalized experiences where machines can understand not just what we say, but also how we feel and what we need.\"\"\"\n",
    "\n",
    "# dividing it into sentences\n",
    "sentences=sent_tokenize(data)\n",
    "stemmer=PorterStemmer()\n",
    "for i in range(len(sentences)):\n",
    "    words=word_tokenize(sentences[i])\n",
    "    data1=[stemmer.stem(x) for x in words if(x not in set(stopwords.words(\"english\")))] \n",
    "    # print(data1)\n",
    "    sentences[i]=\" \".join(str(x) for x in data1)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "112fc943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur languag process becom crucial part modern technolog , influenc almost everi digit interact today .', 'from analyz custom review power intellig assist , nlp help machin understand context , emot , intent behind human communic .', 'over past decad , compani heavili invest nlp research build smarter chatbot , sentiment analysi tool , translat engin , autom content system .', 'what make nlp challeng complex human languageâ€”word multipl mean , grammar rule inconsist , peopl express idea countless way .', 'despit challeng , rapid advanc deep learn larg languag model signific improv accuraci .', 'as busi adopt nlp-driven solut , futur promis high person experi machin understand say , also feel need .']\n"
     ]
    }
   ],
   "source": [
    "# SnowPorter\n",
    "\n",
    "data=\"\"\"Natural Language Processing has become a crucial part of modern technology, influencing almost every digital interaction we have today. From analyzing customer reviews to powering intelligent assistants, NLP helps machines understand the context, emotion, and intent behind human communication. Over the past decade, companies have heavily invested in NLP research to build smarter chatbots, sentiment analysis tools, translation engines, and automated content systems. What makes NLP challenging is the complexity of human languageâ€”words can have multiple meanings, grammar rules can be inconsistent, and people express ideas in countless ways. Despite these challenges, rapid advancements in deep learning and large language models have significantly improved accuracy. As more businesses adopt NLP-driven solutions, the future promises highly personalized experiences where machines can understand not just what we say, but also how we feel and what we need.\"\"\"\n",
    "\n",
    "# dividing it into sentences\n",
    "sentences=sent_tokenize(data)\n",
    "stemmer=SnowballStemmer(\"english\")\n",
    "for i in range(len(sentences)):\n",
    "    words=word_tokenize(sentences[i])\n",
    "    data1=[stemmer.stem(x) for x in words if(x not in set(stopwords.words(\"english\")))] \n",
    "    # print(data1)\n",
    "    sentences[i]=\" \".join(str(x) for x in data1)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51aab315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing become crucial part modern technology , influencing almost every digital interaction today .', 'From analyzing customer review powering intelligent assistant , NLP help machine understand context , emotion , intent behind human communication .', 'Over past decade , company heavily invested NLP research build smarter chatbots , sentiment analysis tool , translation engine , automated content system .', 'What make NLP challenging complexity human languageâ€”words multiple meaning , grammar rule inconsistent , people express idea countless way .', 'Despite challenge , rapid advancement deep learning large language model significantly improved accuracy .', 'As business adopt NLP-driven solution , future promise highly personalized experience machine understand say , also feel need .']\n"
     ]
    }
   ],
   "source": [
    "# SnowPorter\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "data=\"\"\"Natural Language Processing has become a crucial part of modern technology, influencing almost every digital interaction we have today. From analyzing customer reviews to powering intelligent assistants, NLP helps machines understand the context, emotion, and intent behind human communication. Over the past decade, companies have heavily invested in NLP research to build smarter chatbots, sentiment analysis tools, translation engines, and automated content systems. What makes NLP challenging is the complexity of human languageâ€”words can have multiple meanings, grammar rules can be inconsistent, and people express ideas in countless ways. Despite these challenges, rapid advancements in deep learning and large language models have significantly improved accuracy. As more businesses adopt NLP-driven solutions, the future promises highly personalized experiences where machines can understand not just what we say, but also how we feel and what we need.\"\"\"\n",
    "\n",
    "# dividing it into sentences\n",
    "sentences=sent_tokenize(data)\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "for i in range(len(sentences)):\n",
    "    words=word_tokenize(sentences[i])\n",
    "    data1=[lemmatizer.lemmatize(x) for x in words if(x not in set(stopwords.words(\"english\")))] \n",
    "    # print(data1)\n",
    "    sentences[i]=\" \".join(str(x) for x in data1)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6378ade",
   "metadata": {},
   "source": [
    "POS Tagging (Part-of-Speech Tagging) means labeling each word in a sentence with its grammatical role, such as:\n",
    "\n",
    "NN â†’ Noun\n",
    "\n",
    "VB â†’ Verb\n",
    "\n",
    "JJ â†’ Adjective\n",
    "\n",
    "RB â†’ Adverb\n",
    "\n",
    "PRP â†’ Pronoun\n",
    "\n",
    "IN â†’ Preposition\n",
    "\n",
    "DT â†’ Determiner\n",
    "\n",
    "It helps NLP models understand the function of each word.\n",
    "\n",
    "ðŸ§  What is POS Tagging?\n",
    "\n",
    "POS Tagging assigns each word a tag like:\n",
    "\n",
    "\"I am learning NLP\"\n",
    "â†“\n",
    "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('NLP', 'NNP')]\n",
    "\n",
    "\n",
    "So the model knows:\n",
    "\n",
    "\"I\" = pronoun\n",
    "\n",
    "\"am\" = verb\n",
    "\n",
    "\"learning\" = verb\n",
    "\n",
    "\"NLP\" = proper noun\n",
    "\n",
    "This is useful for:\n",
    "âœ” Extracting nouns\n",
    "âœ” Lemmatization\n",
    "âœ” Named Entity Recognition\n",
    "âœ” Chunking\n",
    "âœ” Grammar analysis\n",
    "\n",
    "ðŸ”§ POS Tagging in Python (NLTK)\n",
    "Step 1 â€” Download required models\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "Step 2 â€” Use pos_tag\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "sentence = \"Natural Language Processing makes computers understand human language.\"\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "tags = pos_tag(words)\n",
    "\n",
    "print(tags)\n",
    "\n",
    "Output Example:\n",
    "[('Natural', 'JJ'),\n",
    " ('Language', 'NNP'),\n",
    " ('Processing', 'NNP'),\n",
    " ('makes', 'VBZ'),\n",
    " ('computers', 'NNS'),\n",
    " ('understand', 'VB'),\n",
    " ('human', 'JJ'),\n",
    " ('language', 'NN'),\n",
    " ('.', '.')]\n",
    "\n",
    "ðŸ“˜ Common POStags (Simplified Table)\n",
    "Tag\tMeaning\n",
    "NN\tNoun\n",
    "NNS\tPlural noun\n",
    "NNP\tProper noun\n",
    "VB\tVerb (base)\n",
    "VBD\tVerb (past)\n",
    "VBG\tVerb (gerund / -ing)\n",
    "VBN\tVerb (past participle)\n",
    "VBP\tVerb (non-3rd person present)\n",
    "VBZ\tVerb (3rd person present)\n",
    "JJ\tAdjective\n",
    "RB\tAdverb\n",
    "PRP\tPronoun\n",
    "IN\tPreposition\n",
    "DT\tDeterminer\n",
    "CC\tConjunction\n",
    ".\tPunctuation\n",
    "ðŸŽ¯ Using POS for Lemmatization (Important!)\n",
    "\n",
    "Lemmatization improves when you pass correct POS:\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    return {\n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV\n",
    "    }.get(tag, wordnet.NOUN)\n",
    "\n",
    "word = \"running\"\n",
    "lemma = lemmatizer.lemmatize(word, get_pos(word))\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb303ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sandeep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Sandeep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "24aad454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('makes', 'VBZ'), ('computers', 'NNS'), ('understand', 'JJ'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "sentence = \"Natural Language Processing makes computers understand human language.\"\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "tags = pos_tag(words)\n",
    "\n",
    "print(tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d7255",
   "metadata": {},
   "source": [
    "### Name Entity Recognization\n",
    "\n",
    "Where It will recognize whether it was name or person or any other entity.\n",
    "\n",
    "Named Entity Recognition (NER) is an NLP technique used to identify and classify real-world entities in text.\n",
    "\n",
    "Example entities:\n",
    "\n",
    "PERSON â†’ names of people\n",
    "\n",
    "ORG â†’ organizations\n",
    "\n",
    "GPE â†’ countries, cities, states\n",
    "\n",
    "DATE â†’ time expressions\n",
    "\n",
    "LOC â†’ locations\n",
    "\n",
    "MONEY â†’ amounts\n",
    "\n",
    "PRODUCT â†’ product names\n",
    "\n",
    "ðŸ§  What NER Does\n",
    "\n",
    "Given this sentence:\n",
    "\n",
    "Apple is planning to open a new office in Bangalore by 2025.\n",
    "\n",
    "\n",
    "NER identifies:\n",
    "\n",
    "Apple â†’ ORG\n",
    "\n",
    "Bangalore â†’ GPE\n",
    "\n",
    "2025 â†’ DATE\n",
    "\n",
    "NER helps machines understand who, where, and what you are talking about.\n",
    "\n",
    "ðŸ”¥ What NER is Used For\n",
    "\n",
    "âœ” Chatbots\n",
    "âœ” Resume parsing\n",
    "âœ” Search engines\n",
    "âœ” Text classification\n",
    "âœ” Information extraction\n",
    "âœ” Summarization\n",
    "âœ” News analysis\n",
    "âœ” Medical report analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7225e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Sandeep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Sandeep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sandeep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sandeep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d653771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Barack', 'NNP'), ('Obama', 'NNP'), ('was', 'VBD'), ('born', 'VBN'), ('in', 'IN'), ('Hawaii', 'NNP'), ('and', 'CC'), ('served', 'VBD'), ('as', 'IN'), ('the', 'DT'), ('President', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')]\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker_tab/english_ace_multiclass/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Sandeep/nltk_data'\n    - 'c:\\\\Users\\\\Sandeep\\\\Desktop\\\\2025\\\\ML\\\\NLP\\\\ML_kernal\\\\nltk_data'\n    - 'c:\\\\Users\\\\Sandeep\\\\Desktop\\\\2025\\\\ML\\\\NLP\\\\ML_kernal\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Sandeep\\\\Desktop\\\\2025\\\\ML\\\\NLP\\\\ML_kernal\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Sandeep\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:/nltk_data'\n    - 'C:\\\\Users\\\\Sandeep\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\Sandeep\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\Sandeep\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m pos_tags\u001b[38;5;241m=\u001b[39mpos_tag(tokens)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(pos_tags)\n\u001b[1;32m---> 11\u001b[0m name_entity\u001b[38;5;241m=\u001b[39m\u001b[43mne_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_tags\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sandeep\\Desktop\\2025\\ML\\NLP\\ML_kernal\\lib\\site-packages\\nltk\\chunk\\__init__.py:192\u001b[0m, in \u001b[0;36mne_chunk\u001b[1;34m(tagged_tokens, binary)\u001b[0m\n\u001b[0;32m    190\u001b[0m     chunker \u001b[38;5;241m=\u001b[39m ne_chunker(fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     chunker \u001b[38;5;241m=\u001b[39m \u001b[43mne_chunker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mparse(tagged_tokens)\n",
      "File \u001b[1;32mc:\\Users\\Sandeep\\Desktop\\2025\\ML\\NLP\\ML_kernal\\lib\\site-packages\\nltk\\chunk\\__init__.py:174\u001b[0m, in \u001b[0;36mne_chunker\u001b[1;34m(fmt)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mne_chunker\u001b[39m(fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m    Load NLTK's currently recommended named entity chunker.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMaxent_NE_Chunker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sandeep\\Desktop\\2025\\ML\\NLP\\ML_kernal\\lib\\site-packages\\nltk\\chunk\\named_entity.py:329\u001b[0m, in \u001b[0;36mMaxent_NE_Chunker.__init__\u001b[1;34m(self, fmt)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fmt \u001b[38;5;241m=\u001b[39m fmt\n\u001b[1;32m--> 329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tab_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunkers/maxent_ne_chunker_tab/english_ace_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfmt\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_params()\n",
      "File \u001b[1;32mc:\\Users\\Sandeep\\Desktop\\2025\\ML\\NLP\\ML_kernal\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker_tab/english_ace_multiclass/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Sandeep/nltk_data'\n    - 'c:\\\\Users\\\\Sandeep\\\\Desktop\\\\2025\\\\ML\\\\NLP\\\\ML_kernal\\\\nltk_data'\n    - 'c:\\\\Users\\\\Sandeep\\\\Desktop\\\\2025\\\\ML\\\\NLP\\\\ML_kernal\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Sandeep\\\\Desktop\\\\2025\\\\ML\\\\NLP\\\\ML_kernal\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Sandeep\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:/nltk_data'\n    - 'C:\\\\Users\\\\Sandeep\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\Sandeep\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\Sandeep\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag,word_tokenize,ne_chunk\n",
    "\n",
    "sentence = \"Barack Obama was born in Hawaii and served as the President of the United States.\"\n",
    "\n",
    "tokens=word_tokenize(sentence)\n",
    "\n",
    "pos_tags=pos_tag(tokens)\n",
    "\n",
    "print(pos_tags)\n",
    "\n",
    "name_entity=ne_chunk(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76de25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_kernal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
